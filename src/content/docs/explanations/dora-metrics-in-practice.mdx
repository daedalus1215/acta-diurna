---
title: "DORA Metrics in Practice: Measuring Resilience, Not Perfection"
description: "An applied look at DORA metrics through a real production incident, exploring the tradeoffs between deployment frequency, lead time, change failure rate, and mean time to restore."
---

## Introduction

DORA metrics are often treated as a scorecard—something to optimize toward green without context. In practice, they are more useful as a way to understand **how a system behaves under pressure**, not how often it avoids mistakes.

Our team typically deploys on a two-week cadence. Nearly the entire delivery pipeline is automated, with one notable exception: regression testing remains a thorough, manual process performed by QA. Under normal conditions, this balance works well. It favors safety, reduces risk, and keeps production stable.

Under deadline pressure, however, that balance shifted. To meet delivery needs, we promoted a change to production without waiting for the full regression cycle. A bug was introduced. Production was impacted. And within a few short hours, the issue was detected, fixed, and redeployed. We shipped twice in a single day—once at noon, and again at 4:30 PM.

---

## Why This Incident Matters

This incident highlights a core tension that DORA metrics make visible. Increasing deployment frequency and reducing lead time for changes often requires removing manual gates—such as regression testing. Doing so may increase the likelihood of minor defects reaching production.

Whether that tradeoff is acceptable depends less on avoiding failure entirely and more on **how quickly the system can detect, correct, and recover from it**.

A higher change failure rate is not automatically a sign of poor performance if it is paired with fast, reliable recovery. DORA metrics give us a language to reason about that tradeoff explicitly, rather than treating every defect as an equal failure of process.

---

## Deployment Frequency and Lead Time

Deployment frequency and lead time for changes describe how quickly a team can move code from commit to production. Increasing either often means reducing friction in the delivery pipeline.

Manual regression testing introduces confidence, but it also introduces latency. As deployment frequency increases, the cost of waiting for a full manual pass grows. Without automation, teams are eventually forced to choose between speed and certainty.

This is not a failure of process. It is a natural consequence of system design.

---

## Change Failure Rate Is Not Binary

Change failure rate is often interpreted too simplistically. A failure that causes prolonged downtime is not equivalent to a minor defect that is quickly detected and corrected.

In our case, a defect reached production. That fact matters. But so does the scope of the impact, the speed of detection, and the effort required to correct it.

DORA metrics encourage evaluating failure **in context**, not as a binary success or failure signal.

---

## Mean Time to Restore as a Safety Net

Mean time to restore (MTTR) is the metric that makes speed survivable.

Our ability to deploy twice in one day was not an accident. It was the result of automation, confidence in the deployment pipeline, and familiarity with recovery workflows. MTTR transformed what could have been a prolonged incident into a short-lived disruption.

DORA metrics do not ask teams to eliminate risk. They ask whether the system can **absorb and recover from it**.

---

## The Regression Testing Tradeoff

If we want to sustainably reduce lead time for changes and increase deployment frequency, automated regression testing becomes increasingly important. Without it, minor defects are more likely to escape into production.

However, it may be acceptable—at least temporarily—to tolerate a higher change failure rate if the system can detect issues quickly and restore service reliably. This is not an argument against automation. It is an acknowledgment that tradeoffs exist, and they must be made deliberately.

The failure is not choosing speed or safety. The failure is pretending there is no tradeoff at all.

---

## What This Means for Our Team

DORA metrics help us reason about engineering performance as a system of constraints rather than a checklist of goals.

We can choose to invest in automated regression testing to reduce change failure rate. We can also choose to accept a higher failure rate if our deployment frequency and mean time to restore remain strong. What matters is that these choices are explicit, observable, and grounded in data.

DORA metrics give us a shared language to have those conversations—especially when things go wrong.
